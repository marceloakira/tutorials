# Integrador Redis II: Sincroniza√ß√£o de Dados entre Sistemas de Persist√™ncia - Transforma√ß√£o de Modelos com Apache Camel

## 1. Introdu√ß√£o

Este documento √© parte de uma s√©rie de tutoriais sobre integra√ß√£o de sistemas utilizando Redis como canal de mensagens. Este tutorial √© a continua√ß√£o do Integrador Redis I, onde abordamos a arquitetura de integra√ß√£o e um exemplo b√°sico de sincroniza√ß√£o de dados. No Integrador Redis II, expandimos essa funcionalidade para incluir a transforma√ß√£o de modelos de dados utilizando o Apache Camel.

Relembrando a arquitetura do Integrador Redis I:

![Arquitetura Integrador Redis I](../integrador-redis/diagrama-componentes.png)

Figura 1: Arquitetura do Integrador Redis I

* **Redis Listener**: Um componente que escuta eventos de inser√ß√£o, atualiza√ß√£o e exclus√£o de dados no Redis.
* **Transformador de Modelos**: Um componente respons√°vel por transformar os dados recebidos do Redis em um formato compat√≠vel com o sistema de persist√™ncia de destino (ORM/SQLite e ODM/MongoDB).

### 1.1. Problemas

A transforma√ß√£o de dados √© um aspecto cr√≠tico em sistemas de integra√ß√£o, especialmente quando diferentes sistemas de persist√™ncia utilizam modelos de dados distintos. No Integrador Redis II, enfrentamos os seguintes problemas:

1. **Incompatibilidade de Modelos**: Os dados provenientes de diferentes fontes podem ter estruturas diferentes, o que dificulta a integra√ß√£o.
2. **Complexidade na Transforma√ß√£o**: A l√≥gica de transforma√ß√£o pode ser complexa, exigindo um mapeamento cuidadoso entre os modelos de origem e destino.
3. **Desempenho**: A transforma√ß√£o de dados em tempo real pode impactar o desempenho do sistema, especialmente com grandes volumes de dados.

Para facilitar a transforma√ß√£o de modelos, h√° diversas abordagens e ferramentas dispon√≠veis. Neste tutorial, utilizaremos o Apache Camel, uma ferramenta de integra√ß√£o que facilita a implementa√ß√£o de diversos padr√µes de integra√ß√£o, denominados como Enterprise Integration Patterns (EIPs).

## 1.2. Outros problemas

Outros problemas de integra√ß√£o que n√£o ser√£o tratados neste tutorial, mas que s√£o importantes considerar em um sistema de integra√ß√£o completo, incluem:

1. **Manuten√ß√£o**: A l√≥gica de transforma√ß√£o deve ser facilmente mantida e atualizada conforme os modelos evoluem.
2. **Sincroniza√ß√£o de Dados**: Garantir que os dados transformados sejam sincronizados corretamente entre os sistemas de persist√™ncia. Ex.
   - Inser√ß√£o de dados no Redis deve refletir na base de dados ORM/SQLite e ODM/MongoDB.
   - Se um dos sistemas de persist√™ncia falhar, o sistema deve ser capaz de lidar com a inconsist√™ncia de dados. Exemplos:
     - Se o Redis falhar, os dados devem ser persistidos no Redis quando ele voltar.
     - Se o ORM/SQLite falhar, os dados devem ser persistidos no Redis e sincronizados quando o ORM/SQLite voltar.
     - Se o ODM/MongoDB falhar, os dados devem ser persistidos no Redis e sincronizados quando o ODM/MongoDB voltar.
   - Se algum dado for alterado manualmente em um dos sistemas de persist√™ncia, o integrador deve ser capaz de detectar e sincronizar essas altera√ß√µes. Exemplos:
     - Se um dado for alterado no ORM/SQLite, o integrador deve atualizar o Redis e o ODM/MongoDB.
     - Se um dado for alterado no ODM/MongoDB, o integrador deve atualizar o Redis e o ORM/SQLite.

Os problemas de manuten√ß√£o e sincroniza√ß√£o de dados s√£o complexos e exigem uma abordagem cuidadosa para garantir a integridade e a consist√™ncia dos dados em todo o sistema. Neste tutorial, focaremos na transforma√ß√£o de modelos, mas √© importante ter em mente esses outros aspectos ao projetar um sistema de integra√ß√£o robusto.

## 2. Padr√µes de Projeto de Integra√ß√£o

Problemas de integra√ß√£o de dados s√£o recorrentes de Engenharia de Software e a comunidade desenvolvedora com frequ√™ncia prop√µe solu√ß√µes para esses problemas atrav√©s de padr√µes de projeto. H√° diversas publica√ß√µes na forma de livros, artigos e blogs que abordam esses padr√µes. Um dos mais conhecidos √© o livro ["Enterprise Integration Patterns" de Gregor Hohpe e Bobby Woolf](#hohpe2004), que descreve uma s√©rie de padr√µes para resolver problemas comuns de integra√ß√£o.

Gregor Hohpe mant√©m um portal com uma lista de padr√µes de integra√ß√£o, que pode ser acessado em [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com/). Ele definiu [65 padr√µes de integra√ß√£o](https://www.enterpriseintegrationpatterns.com/patterns/messaging/), que s√£o divididos em categorias como:
* Os **Padr√µes de Canal** descrevem como as mensagens s√£o transportadas atrav√©s de um Canal de Mensagens.
* Os **Padr√µes de Constru√ß√£o de Mensagens** descrevem a inten√ß√£o, a forma e o conte√∫do das mensagens que trafegam pelo sistema de mensagens.
* Os **Padr√µes de Roteamento** discutem como as mensagens s√£o direcionadas de um remetente para o(s) destinat√°rio(s) correto(s).
* Os **Padr√µes de Transforma√ß√£o** alteram o conte√∫do de uma mensagem, por exemplo, para acomodar diferentes formatos de dados usados pelos sistemas de envio e recebimento. Dados podem precisar ser adicionados, removidos ou os dados existentes podem precisar ser reorganizados. O padr√£o base para esta se√ß√£o √© o Tradutor de Mensagens.
* Os **Padr√µes de Endpoint** descrevem como as aplica√ß√µes produzem ou consomem mensagens.
* Os **Padr√µes de Gerenciamento de Sistemas** descrevem o que √© necess√°rio para manter um sistema complexo baseado em mensagens funcionando de forma robusta.

Padr√µes de Integra√ß√£o s√£o uma forma de documentar solu√ß√µes para problemas comuns de integra√ß√£o, permitindo que desenvolvedores e arquitetos de software reutilizem solu√ß√µes comprovadas em seus projetos. A seguir, apresentamos os padr√µes de integra√ß√£o utilizados neste tutorial e os que ser√£o implementados.

## 2.1. Padr√µes de Integra√ß√£o Utilizados

H√° diversos livros e artigos que abordam e catalogam padr√µes de integra√ß√£o. Um dos mais conhecidos √© o livro ["Enterprise Integration Patterns" de Gregor Hohpe e Bobby Woolf](#hohpe2004), que descreve uma s√©rie de padr√µes para resolver problemas comuns de integra√ß√£o. Abaixo, apresentamos uma imagem que ilustra a linguagem de padr√µes de integra√ß√£o proposta por Hohpe e Woolf:

![integration pattern language](integration-pattern-language.png)
Figura 2: Linguagem de Padr√µes de Integra√ß√£o - Fonte: [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com/)

No estudo de caso desta s√©rie de tutoriais, diversos padr√µes de integra√ß√£o s√£o utilizados, incluindo:
* **[Mensagem de Evento (Event Message)](https://www.enterpriseintegrationpatterns.com/patterns/messaging/EventMessage.html)**: Representa uma notifica√ß√£o de que algo aconteceu em um sistema, como a inser√ß√£o, atualiza√ß√£o ou exclus√£o de dados.
* **[Canal Publicador-Assinante (Publisher-Subscriber Channel)](https://www.enterpriseintegrationpatterns.com/patterns/messaging/PublishSubscribeChannel.html)**: Um canal de mensagens onde os remetentes publicam mensagens e os destinat√°rios se inscrevem para receber essas mensagens.
* **[Consumidor Orientado a Evento (Event-Driven Consumer)](https://www.enterpriseintegrationpatterns.com/patterns/messaging/EventDrivenConsumer.html)**: Um consumidor que reage a eventos recebidos atrav√©s de um canal de mensagens.

## 2.2. Padr√µes de Integra√ß√£o a serem implementados

Al√©m dos padr√µes de integra√ß√£o relacionados ao canal e constru√ß√£o de mensagem, h√° outros padr√µes descritos por Hohpe e Woolf para roteamento e transforma√ß√£o de mensagens. Os padr√µes de integra√ß√£o que ser√£o utilizados neste tutorial incluem:
* **[Roteador de Mensagens (Message Router)](https://camel.apache.org/manual/latest/message-router.html)**: Um componente que direciona mensagens para diferentes destinos com base em regras definidas.
* **[Transformador de Mensagens (Message Translator)](https://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html)**: Um componente que transforma o conte√∫do de uma mensagem de um formato para outro, permitindo a compatibilidade entre diferentes sistemas de persist√™ncia.
* **[Modelo de Dados Can√¥nico (Canonical Data Model)](https://www.enterpriseintegrationpatterns.com/patterns/messaging/CanonicalDataModel.html)**: Um modelo de dados comum que serve como intermedi√°rio entre diferentes sistemas, evitando o acoplamento de modelos entre si.

Os tr√™s padr√µes de integra√ß√£o mencionados acima ser√£o utilizados e implementados neste estudo de caso. O Roteador de Mensagens permite direcionar as mensagens para o transformador correto, enquanto o Transformador de Mensagens realiza a transforma√ß√£o propriamente dita. O Modelo de Dados Can√¥nico serve como um intermedi√°rio entre os diferentes sistemas, garantindo que as mensagens sejam compat√≠veis entre si.

## 3. Modelagem do Estudo de Caso

### 3.1. Diagrama de Classes

Considerando que o objetivo do integrador √© transformar modelos de dados entre diferentes sistemas de persist√™ncia, s√£o necess√°rios dois diagramas de classes: um para o modelo baseado em ORM/SQLite e outro para o modelo baseado em ODM/MongoDB.

Como estudo de caso, considere dois sistemas com camadas de persist√™ncia distintas:

* **Sistema de Gest√£o Acad√™mica (SGA)**: respons√°vel pela gest√£o dos processos acad√™micos de uma universidade. Entre suas principais entidades est√£o: *Estudante*, *Disciplina*, *Turma* e *Matr√≠cula*.

* **Sistema de Biblioteca (SB)**: respons√°vel pela gest√£o dos empr√©stimos de livros aos estudantes. Suas principais entidades incluem: *Estudante*, *Livro* e *Empr√©stimo*.

Para fins de integra√ß√£o, a entidade *Estudante* deve ser compartilhada entre os dois sistemas. No SB, o estudante deve possuir um *status de matr√≠cula* que determina se ele est√° autorizado a realizar empr√©stimos. J√° no SGA, o estudante deve possuir um atributo que indica o *status de empr√©stimo de livros* (por exemplo, `QUITADO` ou `EM_ABERTO`), o qual impacta diretamente na autoriza√ß√£o para emiss√£o do diploma.

Abaixo est√£o os diagramas de classes para cada sistema:

### 3.2. Modelo SGA: ORM/SQLite

![Modelo ORM/SQLite](modelo_orm_sqlite.png)

Figura 3: Diagrama de Classes do Modelo SGA, implementado com ORM/SQLite

O c√≥digo-fonte do diagrama de classes do modelo ORM/SQLite pode ser encontrado em [modelo_orm_sqlite.puml](modelo_orm_sqlite.puml).

* **Estudante**: Representa um aluno da institui√ß√£o. Possui um relacionamento "1:N" com a entidade *Matricula*, ou seja, um estudante pode se matricular em v√°rias turmas. Tamb√©m possui um atributo *statusEmprestimoLivros*, que indica a situa√ß√£o do estudante quanto a pend√™ncias com a biblioteca (ex: `QUITADO`, `EM_ABERTO`).
* **Disciplina**: Representa um componente curricular. Possui um relacionamento "1:N" com a entidade *Turma*, indicando que uma disciplina pode ser oferecida em v√°rias turmas.
* **Turma**: Representa uma oferta espec√≠fica de uma disciplina. Possui um relacionamento "N:1" com *Disciplina* e "1:N" com *Matricula*, ou seja, cada turma pertence a uma disciplina e pode ter v√°rias matr√≠culas.
* **Matricula**: Representa o v√≠nculo entre um estudante e uma turma. Possui relacionamentos "N:1" com *Estudante* e *Turma*, e "1:1" com *StatusMatricula*. Ou seja, a matr√≠cula estabelece uma rela√ß√£o N:M entre estudantes e turmas, com um status associado a cada v√≠nculo.
* **StatusMatricula**: Enumera√ß√£o que indica o estado de uma matr√≠cula (ex: `ATIVA`, `TRANCADA`, `CANCELADA`). Cada matr√≠cula possui exatamente um status associado.
* **StatusEmprestimo**: Enumera√ß√£o que indica a situa√ß√£o dos empr√©stimos de livros do estudante, usada para fins administrativos (ex: impedir emiss√£o de diploma). Os valores poss√≠veis s√£o `QUITADO` e `EM_ABERTO`.


### 3.3. Modelo SB: ODM/MongoDB

![Modelo ODM/MongoDB](modelo_odm_mongodb.png)

Figura 4: Diagrama de Classes do Modelo SB, implementado com ODM/MongoDB

O c√≥digo-fonte do diagrama de classes do modelo do Sistemas de Bibliotecas (ODM/MongoDB) pode ser encontrado em [modelo_odm_mongodb.puml](modelo_odm_mongodb.puml).

* **Usuario**: Representa um estudante cadastrado no sistema de biblioteca. O campo `id` √© uma `String` que armazena o valor do `ObjectId` do MongoDB (formato hash), usado como identificador √∫nico do documento. O nome completo do usu√°rio √© representado de forma separada pelos campos `prenome` e `sobrenome`, permitindo maior flexibilidade para ordena√ß√£o, buscas e formata√ß√£o. O campo `situacaoMatricula` indica a situa√ß√£o acad√™mica do usu√°rio (por exemplo: ATIVO, INATIVO), utilizada para autorizar ou restringir empr√©stimos.

* **RegistroEmprestimo**: Representa o empr√©stimo de uma obra a um usu√°rio. Armazena o c√≥digo do empr√©stimo e as datas de in√≠cio e previs√£o de devolu√ß√£o. Cada registro de empr√©stimo est√° associado a um √∫nico usu√°rio e a uma √∫nica obra.

* **Obra**: Representa um livro ou material dispon√≠vel para empr√©stimo. Cont√©m dados bibliogr√°ficos como o c√≥digo, t√≠tulo principal, autor principal e n√∫mero ISBN. Uma mesma obra pode ser associada a v√°rios registros de empr√©stimo ao longo do tempo.


### 3.4. Modelo de Dados Can√¥nico

Para facilitar a transforma√ß√£o entre os modelos ORM/SQLite e ODM/MongoDB, √© necess√°rio definir um Modelo de Dados Can√¥nico (MDC) que sirva como intermedi√°rio. O MDC deve conter os atributos comuns entre os dois modelos, permitindo que as transforma√ß√µes sejam realizadas de forma eficiente.

O MDC √© um padr√£o de integra√ß√£o que minimiza as depend√™ncias entre os sistemas de persist√™ncia, evitando-se que diversos sistemas dependam diretamente uns dos outros. Em vez disso, todos os sistemas dependem do MDC, que atua como um intermedi√°rio entre eles. A figura abaixo ilustra o MDC proposto por [Hohpe e Woolf (2004)](#hohpe2004):

![Modelo de Dados Can√¥nico](CanonicalDataModel.gif)

Figura 5: Modelo de Dados Can√¥nico - Fonte: [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com/patterns/messaging/CanonicalDataModel.html)

Na figura acima, os sistemas A, B e C e D dependem do MDC, mas cada um n√£o dependem diretamente dos outros. Isso permite que os sistemas sejam desacoplados, facilitando a manuten√ß√£o e evolu√ß√£o dos modelos de dados.

### 3.5. üß© Tabela Comparativa: `Estudante` (SGA) √ó `Usuario` (SB)

| Atributo em `Estudante` (SGA)     | Atributo em `Usuario` (SB)     | Equival√™ncia Sem√¢ntica | Observa√ß√µes                                                                 |
|----------------------------------|--------------------------------|--------------------------|------------------------------------------------------------------------------|
| `id` (int)                       | `id` (String)                  | ‚úÖ Sim                   | Ambos representam identificadores √∫nicos; no SB, √© um hash (ObjectId) em String |
| `nomeCompleto` (String)          | `prenome` + `sobrenome`        | ‚úÖ Sim                   | `nomeCompleto` pode ser reconstru√≠do a partir da concatena√ß√£o dos dois campos |
| `dataDeNascimento` (String)     | *(ausente)*                    | ‚ö†Ô∏è Parcial               | Presente apenas no SGA                                                       |
| `matricula` (int)                | *(ausente)*                    | ‚ö†Ô∏è Parcial               | Pode ser derivada ou ignorada, dependendo das regras do integrador           |
| `statusEmprestimoLivros` (enum) | *(ausente)*                    | ‚ùå N√£o                   | Campo espec√≠fico do SGA usado para controle de pend√™ncias com a biblioteca   |
| *(ausente)*                      | `situacaoMatricula` (String)   | ‚ùå N√£o                   | Campo espec√≠fico do SB usado para liberar ou bloquear empr√©stimos            |

### 3.6. Identificador Can√¥nico e Entidade de Mapeamento

Para que o Modelo de Dados Can√¥nico (MDC) cumpra seu papel de forma eficaz, √© fundamental que cada entidade integrada entre os sistemas possua um **identificador universal**, denominado `idCanonico`. Esse identificador atua como chave prim√°ria no dom√≠nio can√¥nico e deve ser **independente dos identificadores locais** usados por cada sistema de origem ou destino.

A ado√ß√£o de um `idCanonico` baseado em **[UUID (Universally Unique Identifier)](https://pt.wikipedia.org/wiki/UUID)** traz benef√≠cios importantes: promove o **desacoplamento entre os sistemas**, assegura **unicidade global** e permite que o integrador opere de forma neutra em rela√ß√£o √†s tecnologias de persist√™ncia utilizadas. Enquanto isso, os sistemas originais ‚Äî como o SGA (com `id` inteiro) e o SB (com `id` em formato de hash) ‚Äî continuam utilizando seus pr√≥prios identificadores.

Para permitir a correspond√™ncia entre esses identificadores locais e o can√¥nico, define-se uma **entidade de mapeamento de IDs**, chamada `MapeamentoID`. Essa entidade relaciona o `idCanonico` aos identificadores espec√≠ficos de cada sistema (`idSGA`, `idSB`) e registra o momento da √∫ltima sincroniza√ß√£o. Ela permite que o integrador **localize, atualize e reconcilie entidades** de forma segura e rastre√°vel. Um benef√≠cio central dessa abordagem √© o suporte √† **[idempot√™ncia](https://www.enterpriseintegrationpatterns.com/patterns/messaging/IdempotentReceiver.html)**, ou seja, a garantia de que opera√ß√µes repetidas n√£o causar√£o efeitos duplicados ‚Äî caracter√≠stica essencial em cen√°rios com reprocessamentos, mensagens duplicadas ou falhas tempor√°rias.

Al√©m disso, o uso do `idCanonico` junto √† entidade `MapeamentoID` torna o modelo naturalmente **extens√≠vel**: caso um novo sistema venha a ser integrado, basta adicionar um novo campo de identificador ao mapeamento e os atributos relevantes ao MDC, sem necessidade de alterar os sistemas existentes. Isso torna a arquitetura preparada para evoluir de forma sustent√°vel e desacoplada.

### 3.7. Modelo de Dados Can√¥nico Proposto

Com base na an√°lise comparativa entre os modelos de dados dos sistemas SGA e SB, prop√µe-se um Modelo de Dados Can√¥nico (MDC) que representa uma abstra√ß√£o comum da entidade *Estudante*. O MDC unifica os atributos compartilhados, normaliza diferen√ßas estruturais e acomoda informa√ß√µes espec√≠ficas de cada sistema, permitindo uma transforma√ß√£o eficiente e desacoplada.

A estrutura proposta inclui atributos como `idCanonico`, que √© um identificador universal baseado em UUID, e os campos `prenome` e `sobrenome`, derivados da separa√ß√£o do `nomeCompleto` utilizado no SGA. Atributos relevantes para ambos os dom√≠nios, como `statusBiblioteca` (proveniente do SGA) e `statusAcademico` (presente no SB), tamb√©m s√£o incorporados ao modelo, assegurando uma vis√£o unificada do estudante em diferentes contextos.

Al√©m disso, para viabilizar a correspond√™ncia entre os identificadores locais dos sistemas e o identificador can√¥nico, define-se uma entidade auxiliar denominada `MapeamentoID`. Essa entidade registra os v√≠nculos entre `idCanonico`, `idSGA` e `idSB`, al√©m de armazenar a data da √∫ltima sincroniza√ß√£o. Esse componente √© essencial para garantir rastreabilidade, consist√™ncia e idempot√™ncia na integra√ß√£o entre os sistemas.

A seguir, apresenta-se o diagrama de classes do modelo can√¥nico proposto, incluindo a entidade de mapeamento de IDs:

![Modelo de Dados Can√¥nico](modelo_de_dados_canonico.png)

Figura 6: Diagrama de Classes do Modelo de Dados Can√¥nico, c√≥digo-fonte dispon√≠vel em [modelo_de_dados_canonico.puml](modelo_de_dados_canonico.puml).

## 4. Implementa√ß√£o

Como foi visto anteriormente, a arquitetura utilizada no integrador √© publicador-assinante, onde o Redis atua como canal de mensagens. O SGA e o SB publicam eventos de inser√ß√£o, atualiza√ß√£o e exclus√£o de dados no Redis, e um consumidor de eventos √© respons√°vel por processar esses eventos e realizar as transforma√ß√µes necess√°rias entre os modelos ORM/SQLite e ODM/MongoDB.

Portanto, a implementa√ß√£o do integrador envolve as seguintes etapas:
1. **Adapta√ß√£o do SGA e SB para publicar eventos CRUD no Redis**: Modificar os sistemas SGA e SB para que publiquem eventos de inser√ß√£o, atualiza√ß√£o e exclus√£o de dados no Redis.
2. **Implementa√ß√£o do consumidor de eventos**: Criar um componente que consome os eventos publicados no Redis e realiza as transforma√ß√µes necess√°rias entre os modelos ORM/SQLite e ODM/MongoDB.
3. **Implementa√ß√£o dos transformadores ORM -> ODM e ODM -> ORM**: Criar transformadores que convertem os dados entre os modelos ORM/SQLite e ODM/MongoDB, garantindo que as informa√ß√µes sejam corretamente mapeadas entre os sistemas.

O c√≥digo-fonte do integrador pode ser encontrado na [pasta code do reposit√≥rio integrador-redis2](https://github.com/marceloakira/tutorials/tree/main/integrador-redis2/code). Foram criados 5 projetos Maven independentes, cada um com seu pr√≥prio `pom.xml` e estrutura de diret√≥rios.

![C√≥digo-Fonte do Integrador Redis II](codigo-fonte.png)

Cada projeto possui uma fun√ß√£o espec√≠fica:
* **sb**: Implementa o Sistema de Biblioteca (SB) com a camada de persist√™ncia ODM/MongoDB.
* **sga**: Implementa o Sistema de Gest√£o Acad√™mica (SGA) com a camada de persist√™ncia ORM/SQLite.
* **publicador**: Implementa o m√≥dulo de publica√ß√£o de eventos no Redis, que ser√° utilizado pelo SGA e SB.
* **reposit√≥rio**: Implementa o reposit√≥rio gen√©rico que realiza opera√ß√µes CRUD e publica eventos no Redis.
* **integrador**: Implementa o integrador que consome os eventos do Redis e realiza as transforma√ß√µes entre os modelos de dados.

As instru√ß√µes de compila√ß√£o e execu√ß√£o dos projetos est√£o dispon√≠veis no arquivo `README.md` da pasta do c√≥digo-fonte. A seguir, detalharemos as etapas de adapta√ß√£o dos sistemas SGA e SB para publicar eventos no Redis, a implementa√ß√£o do consumidor de eventos e os transformadores de modelos.

### 4.1. Adapta√ß√£o para a Produ√ß√£o de Eventos

Para que o SGA e o SB publiquem eventos de inser√ß√£o, atualiza√ß√£o e exclus√£o de dados no Redis, √© necess√°rio adaptar os m√≥dulos de cada sistema para incluir a l√≥gica de publica√ß√£o de eventos. Essa adapta√ß√£o envolve a implementa√ß√£o de um publicador que envia mensagens para um canal espec√≠fico do Redis sempre que uma opera√ß√£o CRUD √© realizada.

Outro ponto importante √© garantir que os eventos sejam publicados de forma ass√≠ncrona, para n√£o bloquear as opera√ß√µes dos sistemas. O Redis oferece suporte a publicadores-assinantes, permitindo que os sistemas publiquem eventos em canais espec√≠ficos e que o consumidor de eventos escute esses canais para processar as mensagens. H√° outras ferramentas que tamb√©m podem ser utilizadas para publicar eventos, como Apache Kafka, RabbitMQ, ActiveMQ, etc. No entanto, neste tutorial, optamos por utilizar o Redis como op√ß√£o de banco de dados NoSQL mais aderente ao prop√≥sito da disciplina.

Outro m√≥dulo que ser√° criado √© o reposit√≥rio, que ser√° respons√°vel por persistir os dados no Redis. O reposit√≥rio deve ser capaz de lidar com as opera√ß√µes CRUD e publicar os eventos correspondentes no canal do Redis atrav√©s do m√≥dulo publicador. Este m√≥dulo √© gen√©rico, ou seja, realiza opera√ß√µes CRUD para qualquer entidade, inicialmente desenvolvido para o SGA em [tutorial anterior](https://github.com/marceloakira/tutorials/tree/main/javafx-crud2#parte-1-reuso-na-camada-modelo). 

#### M√≥dulo Publicador

O m√≥dulo publicador foi explicado no [tutorial anterior](https://github.com/marceloakira/tutorials/tree/main/integrador-redis). Foi implementado usando GSon como serializador/desserializador de objetos Java para JSON e Lettuce como cliente ass√≠ncrono para o Redis.

#### M√≥dulo Reposit√≥rio

O m√≥dulo reposit√≥rio foi implementado em [tutorial anterior](https://github.com/marceloakira/tutorials/tree/main/javafx-crud2) e corresponde √†s classes `Reposit√≥rio` e `Database`. A diferen√ßa √© que o reposit√≥rio pode publicar eventos no Redis atrav√©s do m√≥dulo publicador, por meio do m√©todo `publishCrudOperation(CrudOperation.OperationType operationType, T entity)`:

```java
    /**
     * Publica uma opera√ß√£o CRUD no Redis.
     * @param operationType Tipo da opera√ß√£o (CREATE, UPDATE, DELETE)
     * @param entity Entidade envolvida na opera√ß√£o
     */
    private void publishCrudOperation(CrudOperation.OperationType operationType, T entity) {
        try {
            String entityName = entityClass.getSimpleName();
            String entityJson = gson.toJson(entity);
            String timestamp = LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_DATE_TIME);
            
            CrudOperation operation = new CrudOperation(
                entityName, 
                operationType, 
                CrudOperation.Source.ORM, 
                entityJson, 
                timestamp
            );
            redisPublisher.publishOperation(operation);
        } catch (Exception e) {
            System.err.println("Erro ao publicar opera√ß√£o no Redis: " + e.getMessage());
        }
    }
```

Desta forma, sempre que uma opera√ß√£o CRUD for realizada no reposit√≥rio, um evento correspondente ser√° publicado no Redis. O evento cont√©m informa√ß√µes sobre o tipo de opera√ß√£o (inser√ß√£o, atualiza√ß√£o ou exclus√£o), a entidade afetada e um timestamp. Para que o evento seja publicado, os m√©todos create, update e delete do reposit√≥rio devem chamar o m√©todo `publishCrudOperation` ap√≥s a opera√ß√£o ser conclu√≠da com sucesso. Segue o c√≥digo do m√©todo `create` como exemplo:

```java
    /**
     * Cria uma nova entidade no banco de dados.
     * @param entity Entidade a ser criada
     * @return Entidade criada
     */
    public T create(T entity) {
        try {
            System.out.println("üîç Criando entidade: " + entity);
            int nrows = dao.create(entity);
            if (nrows == 0)
                throw new SQLException("Error: object not saved");
            this.loadedEntity = entity;
            loadedEntities.add(entity);
            
            // Publicar opera√ß√£o CREATE no Redis
            if (enableCrudPublishing) {
                System.out.println("‚úÖ Entidade criada: " + entity);
                System.out.println("  - Operation: CREATE");
                publishCrudOperation(CrudOperation.OperationType.CREATE, entity);
            }
        } catch (SQLException e) {
            System.out.println(e);
            e.printStackTrace();
        }
        return entity;
    }
```

O atributo `enableCrudPublishing` deve ser configurado como `true` para que os eventos sejam publicados no Redis. Caso contr√°rio, as opera√ß√µes CRUD ser√£o realizadas normalmente, mas sem a publica√ß√£o dos eventos.

Como o objetivo deste tutorial √© demonstrar a integra√ß√£o entre os sistemas SGA e SB, o m√≥dulo reposit√≥rio foi adaptado para publicar eventos no Redis. Portanto, neste tutorial as classes `Reposit√≥rio` e `Database` foram removidas da camada model do [tutorial anterior](https://github.com/marceloakira/tutorials/tree/main/javafx-crud2). As classes `Estudante`, `Disciplina`, dentre outras continuam intactas, sem modifica√ß√µes.

### 4.2. Verificando a Publica√ß√£o de Eventos no Redis

Para verificar se os eventos est√£o sendo publicados corretamente no Redis, voc√™ pode usar o servi√ßo de escuta de eventos do Redis - `RedisListenerServer` - apresentado no [tutorial anterior](https://github.com/marceloakira/tutorials/tree/main/javafx-crud2). Para execut√°-lo, basta clonar o projeto, entrar na pasta `code/publicador` e executar os comandos:

```bash
cd code/publicador
mvn clean compile
mvn exec:java -Dexec.mainClass="br.ufg.inf.publicador.RedisListenerServer"
```

A seguinte mensagem deve ser exibida no console:

```
üü¢ Escutando notifica√ß√µes Redis de forma ass√≠ncrona...
```

Para testar a publica√ß√£o de eventos, foram criados testes unit√°rios para o m√≥dulo reposit√≥rio, que realizam opera√ß√µes CRUD e verificam se os eventos s√£o publicados corretamente no Redis. Os testes est√£o localizados na classe `code/repositorio/src/test/java/br/ufg/inf/repositorio/RepositorioTest`:

```java
package br.ufg.inf.repositorio;

import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.AfterEach;
import static org.junit.jupiter.api.Assertions.*;

import com.j256.ormlite.field.DatabaseField;
import com.j256.ormlite.table.DatabaseTable;

import java.io.File;

/**
 * Testes b√°sicos para as classes Database e Repositorio.
 */
public class RepositorioTest {
    
    private Database database;
    private Repositorio<TestEntity, Long> repositorio;
    private final String testDbName = "test_repositorio.sqlite";
    
    @DatabaseTable(tableName = "test_entity")
    public static class TestEntity {
        @DatabaseField(generatedId = true)
        private Long id;
        
        @DatabaseField
        private String nome;
        
        public TestEntity() {
            // Construtor padr√£o necess√°rio para ORMLite
        }
        
        public TestEntity(String nome) {
            this.nome = nome;
        }
        
        // Getters e setters
        public Long getId() { return id; }
        public void setId(Long id) { this.id = id; }
        
        public String getNome() { return nome; }
        public void setNome(String nome) { this.nome = nome; }
        
        @Override
        public String toString() {
            return "TestEntity{id=" + id + ", nome='" + nome + "'}";
        }
    }
    
    @BeforeEach
    public void setUp() {
        // Remove o arquivo de teste se existir
        File testFile = new File(testDbName);
        if (testFile.exists()) {
            testFile.delete();
        }
        
        database = new Database(testDbName);
        repositorio = new Repositorio<>(database, TestEntity.class);
    }
    
    @AfterEach
    public void tearDown() {
        if (database != null) {
            database.close();
        }
        
        // Remove o arquivo de teste
        File testFile = new File(testDbName);
        if (testFile.exists()) {
            testFile.delete();
        }
    }
    
    @Test
    public void testDatabaseConnection() throws Exception {
        assertNotNull(database.getConnection());
        // Verifica se a conex√£o n√£o √© nula, o que indica que foi criada com sucesso
    }
    
    @Test
    public void testCreateEntity() {
        TestEntity entity = new TestEntity("Teste");
        TestEntity created = repositorio.create(entity);
        
        assertNotNull(created);
        assertNotNull(created.getId());
        assertEquals("Teste", created.getNome());
    }
    
    @Test
    public void testLoadFromId() {
        // Criar uma entidade
        TestEntity entity = new TestEntity("Teste Load");
        TestEntity created = repositorio.create(entity);
        
        // Carregar por ID
        TestEntity loaded = repositorio.loadFromId(created.getId());
        
        assertNotNull(loaded);
        assertEquals(created.getId(), loaded.getId());
        assertEquals("Teste Load", loaded.getNome());
    }
    
    @Test
    public void testLoadAll() {
        // Criar algumas entidades
        repositorio.create(new TestEntity("Entidade 1"));
        repositorio.create(new TestEntity("Entidade 2"));
        repositorio.create(new TestEntity("Entidade 3"));
        
        // Carregar todas
        var entities = repositorio.loadAll();
        
        assertNotNull(entities);
        assertEquals(3, entities.size());
    }
    
    @Test
    public void testUpdate() {
        // Criar uma entidade
        TestEntity entity = new TestEntity("Nome Original");
        TestEntity created = repositorio.create(entity);
        
        // Atualizar
        created.setNome("Nome Atualizado");
        repositorio.update(created);
        
        // Verificar se foi atualizada
        TestEntity loaded = repositorio.loadFromId(created.getId());
        assertEquals("Nome Atualizado", loaded.getNome());
    }
    
    @Test
    public void testDelete() {
        // Criar uma entidade
        TestEntity entity = new TestEntity("Para Deletar");
        TestEntity created = repositorio.create(entity);
        Long id = created.getId();
        
        // Deletar
        repositorio.delete(created);
        
        // Verificar se foi deletada
        TestEntity loaded = repositorio.loadFromId(id);
        assertNull(loaded);
    }
}
```

O teste √© bastante auto-contido, ou seja, ele configura seu pr√≥prio ambiente, executa as opera√ß√µes necess√°rias e verifica os resultados, tudo dentro de um √∫nico m√©todo de teste. Isso facilita a compreens√£o e manuten√ß√£o dos testes. Uma entidade de teste `TestEntity` foi criada para representar uma entidade simples com um campo `nome`. Os testes cobrem as opera√ß√µes b√°sicas de cria√ß√£o, carregamento, atualiza√ß√£o e exclus√£o de entidades no reposit√≥rio. Um arquivo de banco de dados SQLite chamado `test_repositorio.sqlite` √© criado durante os testes e removido ap√≥s a execu√ß√£o. Os testes garantem que as opera√ß√µes CRUD funcionem corretamente.

Para executar os testes, voc√™ pode usar o Maven. Basta entrar na pasta `code/repositorio` e executar os seguintes comandos:

```bash
cd code/repositorio
mvn clean test
```

Para verificar se os eventos foram publicados no Redis, verifique se mensagens semelhantes a estas aparecem no console do `RedisListenerServer`:

```
üîî [crud-channel] {"entity":"TestEntity","operation":"CREATE","source":"ORM","data":"{\"id\":1,\"nome\":\"Para Deletar\"}","timestamp":"2025-07-27T12:51:39.88406"}
12:51:39.923 [lettuce-nioEventLoop-4-1] DEBUG io.lettuce.core.protocol.CommandHandler -- [channel=0x4200d860, /127.0.0.1:49617 -> localhost/127.0.0.1:6379, epid=0x1, chid=0x1] Received: 190 bytes, 0 commands in the stack
12:51:39.923 [lettuce-nioEventLoop-4-1] DEBUG io.lettuce.core.protocol.RedisStateMachine -- Decode done, empty stack: true
üîî [crud-channel] {"entity":"TestEntity","operation":"DELETE","source":"ORM","data":"{\"id\":1,\"nome\":\"Para Deletar\"}","timestamp":"2025-07-27T12:51:39.908326"}
12:51:39.955 [lettuce-nioEventLoop-4-1] DEBUG io.lettuce.core.protocol.CommandHandler -- [channel=0x4200d860, /127.0.0.1:49617 -> localhost/127.0.0.1:6379, epid=0x1, chid=0x1] Received: 191 bytes, 0 commands in the stack
12:51:39.956 [lettuce-nioEventLoop-4-1] DEBUG io.lettuce.core.protocol.RedisStateMachine -- Decode done, empty stack: true
üîî [crud-channel] {"entity":"TestEntity","operation":"CREATE","source":"ORM","data":"{\"id\":1,\"nome\":\"Nome Original\"}","timestamp":"2025-07-27T12:51:39.935287"}
12:51:39.973 [lettuce-nioEventLoop-4-1] DEBUG io.lettuce.core.protocol.CommandHandler -- [channel=0x4200d860, /127.0.0.1:49617 -> localhost/127.0.0.1:6379, epid=0x1, chid=0x1] Received: 193 bytes, 0 commands in the stack
12:51:39.974 [lettuce-nioEventLoop-4-1] DEBUG io.lettuce.core.protocol.RedisStateMachine -- Decode done, empty stack: true
üîî [crud-channel] {"entity":"TestEntity","operation":"UPDATE","source":"ORM","data":"{\"id\":1,\"nome\":\"Nome Atualizado\"}","timestamp":"2025-07-27T12:51:39.958644"}
```

Caso voc√™ n√£o veja mensagens semelhantes no console do `RedisListenerServer`, verifique se o servidor Redis est√° em execu√ß√£o e se a configura√ß√£o do canal est√° correta. 

Com isso a camada de persist√™ncia do SGA est√° pronta para publicar eventos no Redis. O pr√≥ximo passo √© implementar o consumidor de eventos que ir√° processar esses eventos e realizar as transforma√ß√µes necess√°rias entre os modelos ORM/SQLite e ODM/MongoDB.

### 4.3. Implementa√ß√£o do Consumidor de Eventos

Para consumir eventos, podemos usar algum componente de conex√£o do [Apache Camel](https://camel.apache.org/). Al√©m de conectar com diversos servi√ßos, esta biblioteca de integra√ß√£o tamb√©m facilita a cria√ß√£o de rotas de mensagens e transforma√ß√µes de dados entre diferentes sistemas. H√° uma lista de centenas de componentes dispon√≠veis no [site do Apache Camel](https://camel.apache.org/components). Para consumir eventos do Redis, h√° apenas um componente dispon√≠vel, que √© o [Spring Redis](https://camel.apache.org/components/latest/redis-component.html):

![Spring Redis](spring-redis.png)

Nos testes de implementa√ß√£o, esse componente n√£o funcionou adequadamente, ent√£o optamos por implementar um consumidor de eventos utilizando o cliente ass√≠ncrono Lettuce, que j√° foi utilizado no m√≥dulo publicador. O c√≥digo do consumidor de eventos est√° localizado na classe `br.ufg.inf.integrador.RedisListener`:

```java
public class RedisListener {

    private final ProducerTemplate producer;

    public RedisListener(ProducerTemplate producer) {
        this.producer = producer;
    }

    public void start() {
        RedisClient client = RedisClient.create("redis://localhost:6379");
        StatefulRedisPubSubConnection<String, String> connection = client.connectPubSub();

        connection.addListener(new RedisPubSubAdapter<String, String>() {
            @Override
            public void message(String channel, String message) {
                System.out.printf("üîî [%s] %s%n", channel, message);

                // Envia a mensagem para o Camel
                producer.sendBody("direct:crud", message);
            }
        });

        connection.sync().subscribe("crud-channel");

        System.out.println("üü¢ Escutando canal Redis: crud-channel");
    }
}
```

O consumidor de eventos utiliza o cliente Lettuce para se conectar ao Redis e assinar o canal `crud-channel`. Quando uma mensagem √© recebida, ela √© enviada para o Apache Camel atrav√©s do [ProducerTemplate](https://camel.apache.org/manual/producertemplate.html), que encaminha a mensagem para a rota (endpoint) definida no Camel. A classe `ProducerTemplate` √© respons√°vel por enviar mensagens para uma rota espec√≠fica do Camel, que neste caso √© o endpoint `direct:crud`:

![Producer Template](producer-template.png)

Para executar o consumidor de eventos, voc√™ pode criar uma classe principal que inicializa o Camel e o RedisListener. O c√≥digo da classe principal est√° localizado em `br.ufg.inf.integrador.Main`:

```java
package br.ufg.inf.integrador;

import br.ufg.inf.integrador.redis.RedisListener;
import br.ufg.inf.integrador.routes.CrudTransformRoute;
import org.apache.camel.CamelContext;
import org.apache.camel.impl.DefaultCamelContext;

public class Main {
    public static void main(String[] args) throws Exception {
        CamelContext context = new DefaultCamelContext();
        context.addRoutes(new CrudTransformRoute());
        context.start();

        // Produz mensagem para rota a partir do Redis
        new RedisListener(context.createProducerTemplate()).start();

        Thread.currentThread().join(); // mant√©m aplica√ß√£o viva
    }
}
```

A classe `Main` inicializa o Camel e adiciona a rota de transforma√ß√£o de CRUD, que ser√° explicada na pr√≥xima se√ß√£o. Em seguida, cria uma inst√¢ncia do `RedisListener` e inicia a escuta do canal `crud-channel`. O m√©todo `Thread.currentThread().join()` √© utilizado para manter a aplica√ß√£o em execu√ß√£o, aguardando eventos do Redis.

A rota de transforma√ß√£o foi inclu√≠da no [contexto do Camel](https://camel.apache.org/manual/camelcontext.html). O conceito de contexto do Camel √© fundamental para entender como as rotas e os componentes interagem entre si:

![Camel Context](camel-context.png)

Na pr√≥xima se√ß√£o, veremos como implementar a rota de transforma√ß√£o de CRUD, que ser√° respons√°vel por processar as mensagens recebidas do Redis e realizar as transforma√ß√µes necess√°rias entre os modelos ORM/SQLite e ODM/MongoDB.

### 4.4. Implementa√ß√£o dos Transformadores de Modelos

#### Rota de Transforma√ß√£o de CRUD

O c√≥digo-fonte do transformador de modelos est√° localizado na classe `br.ufg.inf.integrador.routes.CrudTransformRoute`. Esta classe define uma rota do Apache Camel que consome mensagens do endpoint `direct:crud` e realiza as transforma√ß√µes necess√°rias entre os modelos ORM/SQLite e ODM/MongoDB:

```java
public class CrudTransformRoute extends RouteBuilder {

    private final Gson gson = new Gson();

    @Override
    public void configure() {

        from("direct:crud")
            .routeId("crud-transformer")
            .log("üì• Mensagem recebida: ${body}")
            
            // Prepara headers e propriedades
            .process(this::setOperationHeaders)

            // Transforma o corpo JSON original para o formato necess√°rio (ex: Usuario)
            .process(new CrudProcessor())

            .choice()
                .when(exchangeProperty("CamelSkipSendToEndpoint").isNotEqualTo(true))
                    .log("‚úÖ JSON processado: ${body}")
                    .setHeader("Content-Type", constant("application/json;charset=UTF-8"))

                    // Envia para o endpoint determinado dinamicamente
                    .toD("${header.targetEndpoint}")

                    // Loga a resposta HTTP
                    .log("üì§ Resposta HTTP: Status=${header.CamelHttpResponseCode}, Body=${body}")

                    // üîÅ Persist√™ncia local (canonico + mapeamento) apenas para entidade Estudante/CREATE
                    .filter().simple("${exchangeProperty.crudEntity.toLowerCase()} == 'estudante' && ${exchangeProperty.crudOperation} == 'CREATE'")
                        .process(new PersistenciaCanonicoProcessor())
                    .endChoice()

                .otherwise()
                    .log("‚ö†Ô∏è Mensagem ignorada pelo processador");
    }
    ...
}
```

A [rota](https://camel.apache.org/manual/route-builder.html) √© configurada por meio de uma [linguagem de dom√≠nio espec√≠fica (DSL) do Apache Camel](https://camel.apache.org/manual/routes.html), que permite definir as etapas de processamento de forma declarativa. A rota consome mensagens do endpoint `direct:crud`, processa os dados e envia para o endpoint determinado dinamicamente.

Como uma DSL declarativa, a rota √© composta por uma s√©rie de etapas encadeadas, onde cada etapa realiza uma opera√ß√£o espec√≠fica. As etapas s√£o definidas usando [m√©todos](https://docs.redhat.com/en/documentation/red_hat_fuse/7.5/html/apache_camel_development_guide/fusemrstartedblocks) como `from`, `log`, `process`, `choice`, `when`, `otherwise`, entre outros.

#### Processador `setOperationHeaders`

A rota come√ßa consumindo mensagens do endpoint `direct:crud` e registra a mensagem recebida no log. Em seguida, o processador `setOperationHeaders` √© chamado para definir os cabe√ßalhos e propriedades da mensagem, como o tipo de opera√ß√£o (CREATE, UPDATE, DELETE) e a entidade envolvida. O c√≥digo do processador `setOperationHeaders` √© o seguinte:

```java
    /**
     * Processa a mensagem original para extrair informa√ß√µes da opera√ß√£o CRUD
     * e configurar os headers apropriados para o m√©todo HTTP e endpoint
     */
    private void setOperationHeaders(Exchange exchange) throws Exception {
        String json = exchange.getIn().getBody(String.class);
        CrudOperation op = gson.fromJson(json, CrudOperation.class);
        
        // Mapear opera√ß√£o CRUD para m√©todo HTTP
        String httpMethod = mapOperationToHttpMethod(op.getOperation());
        exchange.getIn().setHeader("CamelHttpMethod", httpMethod);
        
        // Mapear entidade para endpoint
        String endpoint = mapEntityToEndpoint(op.getEntity(), op.getOperation());
        exchange.getIn().setHeader("targetEndpoint", endpoint);
        
        // Armazenar informa√ß√µes da opera√ß√£o para uso posterior
        exchange.setProperty("crudOperation", op.getOperation());
        exchange.setProperty("crudEntity", op.getEntity());
        
        System.out.println("Headers configurados - M√©todo: " + httpMethod + ", Endpoint: " + endpoint);
    }
```

Esse processador extrai o corpo da mensagem original, que √© um JSON representando uma opera√ß√£o CRUD, e converte-o em um objeto `CrudOperation` usando o GSon. Em seguida, ele mapeia a opera√ß√£o CRUD para um m√©todo HTTP (GET, POST, PUT, DELETE) e define o endpoint de destino com base na entidade e na opera√ß√£o. As informa√ß√µes da opera√ß√£o s√£o armazenadas como propriedades do [objeto Exchange](https://camel.apache.org/manual/exchange.html) para uso posterior. O conceito de `Exchange` √© fundamental no Apache Camel, pois representa uma mensagem que est√° sendo processada e cont√©m informa√ß√µes sobre o corpo da mensagem, cabe√ßalhos, propriedades e outros metadados.

#### Processador `CrudProcessor`

O pr√≥ximo passo na rota √© processar o corpo da mensagem usando o `CrudProcessor`, que transforma o JSON original no formato necess√°rio para a opera√ß√£o. O c√≥digo do `CrudProcessor` √© o seguinte:

```java
package br.ufg.inf.integrador.processor;

import br.ufg.inf.publicador.CrudOperation;
import com.google.gson.Gson;
import org.apache.camel.Exchange;
import org.apache.camel.Processor;

public class CrudProcessor implements Processor {

    private final Gson gson = new Gson();
    private final EstudanteProcessor estudanteProcessor = new EstudanteProcessor();

    @Override
    public void process(Exchange exchange) throws Exception {
        String json = exchange.getIn().getBody(String.class);
        
        CrudOperation op = gson.fromJson(json, CrudOperation.class);

        // Armazena a opera√ß√£o original no Exchange para uso posterior
        exchange.setProperty("CrudOriginal", op);

        // Verifica se √© uma opera√ß√£o CREATE do ORM
        if (op.getOperation() == CrudOperation.OperationType.CREATE
            && op.getSource() == CrudOperation.Source.ORM) {
            
            // Processar baseado no tipo de entidade
            String resultado = processarEntidade(op);
            
            if (resultado != null) {
                exchange.getIn().setBody(resultado);
            } else {
                exchange.setProperty("CamelSkipSendToEndpoint", true);
            }

        } else {
            exchange.setProperty("CamelSkipSendToEndpoint", true);
        }
    }

    /**
     * Direciona o processamento de acordo com o tipo de entidade
     */
    private String processarEntidade(CrudOperation op) {
        String entidade = op.getEntity();
        
        if ("Estudante".equalsIgnoreCase(entidade)) {
            return estudanteProcessor.estudanteParaUsuario(op.getData());
        }

        // Futuro: suporte a outras entidades como Professor, Disciplina, etc.
        return null; // Entidade n√£o suportada
    }

}
```

O `CrudProcessor` √© respons√°vel por processar a mensagem recebida e transformar o corpo JSON original no formato necess√°rio para a opera√ß√£o. Ele verifica se a opera√ß√£o √© uma cria√ß√£o (`CREATE`) do ORM e, se for, chama o m√©todo `processarEntidade` para direcionar o processamento de acordo com o tipo de entidade. Neste exemplo, apenas a entidade `Estudante` √© suportada, e o `EstudanteProcessor` √© respons√°vel por transformar os dados do estudante no formato de usu√°rio esperado pelo SB.

Note no c√≥digo que o `CrudProcessor` armazena a opera√ß√£o original no `Exchange` para uso posterior e define a propriedade `CamelSkipSendToEndpoint` como `true` se a entidade n√£o for suportada ou se n√£o houver um resultado v√°lido. Isso impede que a mensagem seja enviada para o endpoint, evitando erros.

#### Processador `EstudanteProcessor`

O `EstudanteProcessor` √© uma classe auxiliar que cont√©m a l√≥gica de transforma√ß√£o dos dados do estudante para o formato de usu√°rio e de diversos modelos em formato JSON. A estrutura do c√≥digo do `EstudanteProcessor` √© o seguinte:

```java
package br.ufg.inf.integrador.processor;

import br.ufg.inf.integrador.model.EstudanteCanonico;
import br.ufg.inf.sb.model.Usuario;
import com.google.gson.Gson;
import com.google.gson.JsonObject;
import com.google.gson.JsonParser;

import java.util.UUID;

/**
 * Classe respons√°vel pelas transforma√ß√µes de dados relacionadas ao Estudante.
 * Cont√©m m√©todos reutiliz√°veis para converter dados de Estudante para diferentes formatos.
 */
public class EstudanteProcessor {

    private final Gson gson = new Gson();

    /**
     * Transforma os dados JSON de um estudante em um objeto Usuario (compat√≠vel com o SB).
     * 
     * @param dadosJson JSON contendo os dados do estudante
     * @return JSON do objeto Usuario serializado, ou null em caso de erro
     */
    public String estudanteParaUsuario(String dadosJson) {
      // ...
    }

    /**
     * Transforma os dados JSON de um estudante em um objeto EstudanteCanonico.
     * 
     * @param dadosJson JSON contendo os dados do estudante
     * @param idCanonico ID can√¥nico a ser atribu√≠do ao estudante
     * @return Objeto EstudanteCanonico populado, ou null em caso de erro
     */
    public EstudanteCanonico estudanteParaEstudanteCanonico(String dadosJson, String idCanonico) {
        // ...
    }

    public EstudanteCanonico estudanteParaEstudanteCanonico(String dadosJson) {
        // ...
    }

    public String extrairNomeCompleto(String dadosJson) {
      // ...
    }

    public String[] extrairPrenomeESobrenome(String nomeCompleto) {
      // ...
    }
}
```

#### Mapeamento de Entidade para Endpoint

O pr√≥ximo passo na rota √© enviar a mensagem processada para o endpoint determinado dinamicamente. O m√©todo `toD` √© utilizado para enviar a mensagem para o endpoint configurado no cabe√ßalho `targetEndpoint` do SB. O endpoint depende de qual entidade est√° sendo processada e qual opera√ß√£o est√° sendo realizada. Por exemplo, se a entidade for `Estudante` e a opera√ß√£o for `CREATE`, o endpoint ser√° algo como `http://localhost:8080/usuarios`. A l√≥gica para determinar o endpoint √© implementada no m√©todo `mapEntityToEndpoint` do `CrudProcessor`:

```java
    private String mapEntityToEndpoint(String entity, CrudOperation.OperationType operation) {
        String baseUrl = "http://localhost:8080";
        String endpoint;

        switch (entity.toLowerCase()) {
            case "estudante": endpoint = baseUrl + "/usuarios"; break;
            // case "professor": endpoint = baseUrl + "/professores"; break;
            // case "disciplina": endpoint = baseUrl + "/disciplinas"; break;
            // case "turma": endpoint = baseUrl + "/turmas"; break;
            default: endpoint = baseUrl + "/" + entity.toLowerCase() + "s";
        }

        return endpoint + "?throwExceptionOnFailure=false&bridgeEndpoint=true&charset=UTF-8";
    }
```

O m√©todo `mapEntityToEndpoint` mapeia a entidade para o endpoint correspondente, considerando a opera√ß√£o a ser realizada. Ele constr√≥i a URL base do SB e adiciona o caminho espec√≠fico da entidade. O par√¢metro `throwExceptionOnFailure=false` √© utilizado para evitar que o Camel lance uma exce√ß√£o caso a resposta HTTP n√£o seja bem-sucedida, permitindo que o processamento continue mesmo em caso de falha.

#### Persist√™ncia Local (Canonico + Mapeamento)

Ap√≥s enviar a mensagem para o endpoint, a rota registra a resposta HTTP no log. Em seguida, h√° uma verifica√ß√£o condicional para persistir os dados can√¥nicos e o mapeamento de IDs apenas para a entidade `Estudante` e para opera√ß√µes de cria√ß√£o (`CREATE`). Isso √© feito atrav√©s do processador `PersistenciaCanonicoProcessor`, que √© respons√°vel por persistir os dados can√¥nicos e o mapeamento de IDs no Redis. O c√≥digo do `PersistenciaCanonicoProcessor` √© o seguinte:

```java
// package e importa√ß√µes omitidas para brevidade

public class PersistenciaCanonicoProcessor implements Processor {

    private final Gson gson = new Gson();
    private final EstudanteProcessor estudanteProcessor = new EstudanteProcessor();
    private final Repositorio<EstudanteCanonico, String> estudanteRepo;
    private final Repositorio<EstudanteIdMapping, String> mappingRepo;

    public PersistenciaCanonicoProcessor() {
        Database db = new Database("integrador.db");
        this.estudanteRepo = new Repositorio<>(db, EstudanteCanonico.class);
        this.estudanteRepo.setEnableCrudPublishing(false); // Desabilita publica√ß√£o de CRUD para evitar loops
        this.mappingRepo = new Repositorio<>(db, EstudanteIdMapping.class);
        this.mappingRepo.setEnableCrudPublishing(false); // Desabilita publica√ß√£o de CRUD para evitar loops
    }

    @Override
    public void process(Exchange exchange) throws Exception {
        String entidade = ((String) exchange.getProperty("crudEntity")).toLowerCase();
        CrudOperation.OperationType operacao = (CrudOperation.OperationType) exchange.getProperty("crudOperation");

        if (!"estudante".equals(entidade) || operacao != CrudOperation.OperationType.CREATE) {
            return;
        }

        String resposta = exchange.getMessage().getBody(String.class);
        JsonObject jsonResposta = JsonParser.parseString(resposta).getAsJsonObject();
        String idSB = jsonResposta.has("id") ? jsonResposta.get("id").getAsString() : null;

        CrudOperation op = exchange.getProperty("CrudOriginal", CrudOperation.class);
        String idSGA = null;
        JsonObject json = JsonParser.parseString(op.getData()).getAsJsonObject();
        if (json.has("id")) {
            idSGA = json.get("id").getAsString();
        }

        String idCanonico = UUID.randomUUID().toString();

        // Usa o EstudanteProcessor para criar o EstudanteCanonico
        EstudanteCanonico ec = estudanteProcessor.estudanteParaEstudanteCanonico(op.getData(), idCanonico);
        
        if (ec == null) {
            System.err.println("‚ùå Erro ao criar EstudanteCanonico");
            return;
        }

        EstudanteIdMapping map = new EstudanteIdMapping();
        map.setIdCanonico(idCanonico);
        map.setIdSGA(idSGA);
        map.setIdSB(idSB);
        map.setUltimaAtualizacao(Instant.now().toString());

        estudanteRepo.create(ec);
        mappingRepo.create(map);
    }
}
```

O `PersistenciaCanonicoProcessor` √© respons√°vel por persistir os dados can√¥nicos e o mapeamento de IDs em um banco de dados SQLite. Ele verifica se a entidade √© `Estudante` e se a opera√ß√£o √© `CREATE`. Em seguida, ele extrai o ID do SB da resposta HTTP e o ID do SGA da opera√ß√£o original. Um novo ID can√¥nico √© gerado usando `UUID.randomUUID()`, e o `EstudanteProcessor` √© utilizado para criar um objeto `EstudanteCanonico` a partir dos dados do estudante. O mapeamento de IDs √© armazenado em um objeto `EstudanteIdMapping`, que cont√©m os IDs can√¥nico, SGA e SB, al√©m de um timestamp da √∫ltima atualiza√ß√£o.

#### Modelos de Dados Canonico e Mapeamento

O `EstudanteIdMapping` √© uma classe simples que representa o mapeamento de IDs entre os sistemas SGA e SB. O c√≥digo dessa classe √© o seguinte:

```java
@DatabaseTable(tableName = "estudante_id_mapping")
public class EstudanteIdMapping {

    @DatabaseField(id = true)
    private String idCanonico;

    @DatabaseField
    private String idSGA;

    @DatabaseField
    private String idSB;
```

O atributo `idCanonico` √© o identificador can√¥nico gerado pelo integrador, enquanto `idSGA` e `idSB` s√£o os IDs correspondentes nos sistemas SGA e SB, respectivamente. A classe tamb√©m possui um campo `ultimaAtualizacao`, que armazena a data e hora da √∫ltima atualiza√ß√£o do mapeamento.

O EstudanteCanonico √© uma classe que representa o modelo can√¥nico do estudante, contendo os dados necess√°rios para a integra√ß√£o. O c√≥digo dessa classe √© o seguinte:

```java
@DatabaseTable(tableName = "estudante_canonico")
public class EstudanteCanonico {

    @DatabaseField(id = true)
    private String idCanonico;

    @DatabaseField
    private String prenome;

    @DatabaseField
    private String sobrenome;

    @DatabaseField
    private String nomeCompleto;

    @DatabaseField
    private String dataDeNascimento;

    @DatabaseField
    private String matricula;

    @DatabaseField
    private String statusAcademico;

    @DatabaseField
    private String statusBiblioteca;
```

A classe `EstudanteCanonico` cont√©m os campos necess√°rios para representar um estudante de forma can√¥nica, incluindo o ID can√¥nico, prenome, sobrenome, nome completo, data de nascimento, matr√≠cula e status acad√™mico e de biblioteca. Esses dados s√£o utilizados para persistir as informa√ß√µes do estudante no banco de dados SQLite e tamb√©m para enviar ao SB quando necess√°rio.

## 4.4. Testes

Para testar de forma interativa e manual, vamos executar os sitemas SB, SGA e integrador.

### Executar o Sistema de Bibliotecas (SB)
Para checar se o integrador est√° funcionando √© necess√°rio executar inicialmente o servidor do Sistema de Bibliotecas (SB). Este sistema foi baseado no [tutorial anterior](https://github.com/marceloakira/tutorials/tree/main/java-odm). Portanto, uma inst√¢ncia do MongoDB deve estar em execu√ß√£o, e o SB deve ser iniciado com o comando:

```bash
cd code/sb
mvn clean compile exec:java -Dexec.mainClass="br.ufg.inf.sb.Main"
```

Para verificar se o SB est√° funcionando corretamente, voc√™ pode acessar a interface web do SB em `http://localhost:8080` e verificar se as opera√ß√µes de CRUD est√£o funcionando como esperado:

![SB Localhost](sb-localhost.png)

### Executar o Integrador

Para executar o integrador, voc√™ deve compilar o projeto e iniciar a aplica√ß√£o. O integrador ir√° consumir eventos do Redis e processar as opera√ß√µes de CRUD conforme configurado. Para isso, execute os seguintes comandos na pasta `code/integrador`:

```bash
cd code/integrador
mvn clean compile exec:java -Dexec.mainClass="br.ufg.inf.integrador.Main"
```

Ap√≥s iniciar o integrador, voc√™ ver√° mensagens indicando que ele est√° escutando o canal Redis `crud-channel`:

```üü¢ Escutando canal Redis: crud-channel
```

### Executar um teste de inclus√£o de estudante

Para testar a inclus√£o de um estudante, foi criando uma classe simples `MainTest`:

```java
package br.ufg.inf.integrador;

import br.ufg.inf.sga.model.Estudante;
import br.ufg.inf.sga.model.Repositorios;

public record MainTest() {
    public static void main(String[] args) {
        System.out.println("Running CRUD Event Test...");
        
        // Example instantiation of Estudante
        Estudante estudante = new Estudante();
        estudante.setNomeCompleto("Jo√£o da Silva");
        estudante.setDataDeNascimento("01/01/2000");
        estudante.setMatricula(123456);

        Repositorios.ESTUDANTE.create(estudante);
        // Estudante criado deve aparecer na console do RedisListenerServer
    }    
}
```

Para executar o teste de inclus√£o de estudante, voc√™ pode compilar e executar a classe `MainTest`:

```bash
cd code/integrador
mvn exec:java -Dexec.mainClass="br.ufg.inf.integrador.MainTest"
``` 

Ap√≥s executar o teste, voc√™ deve ver mensagens no console da classe `Main` executada anteriormente:

```bash
14:37:51.149 [lettuce-nioEventLoop-8-1] INFO crud-transformer -- ‚úÖ JSON processado: {"sobrenome":"da Silva","prenome":"Jo√£o","situacaoMatricula":"ATIVO"}
14:37:52.118 [lettuce-nioEventLoop-8-1] INFO crud-transformer -- üì§ Resposta HTTP: Status=200, Body={"id":"688663f03fc00d0db8d5225d","sobrenome":"da Silva","prenome":"Jo√£o","situacaoMatricula":"ATIVO"}
üîç [EstudanteProcessor] EstudanteCanonico criado: idCanonico=d3171836-980a-4e31-ae2d-df45eba1a41e, prenome=Jo√£o, sobrenome=da Silva, nomeCompleto=Jo√£o da Silva
üîç Criando entidade: br.ufg.inf.integrador.model.EstudanteCanonico@729b7370
üîç Criando entidade: br.ufg.inf.integrador.model.EstudanteIdMapping@30752272
‚úÖ EstudanteCanonico persistido no SQLite
‚úÖ EstudanteIdMapping persistido no SQLite
```

Essas mensagens indicam que o integrador processou a inclus√£o do estudante e enviou os dados para o SB, al√©m de persistir os dados can√¥nicos e o mapeamento de IDs no SQLite. Ao recarregar a interface do SB, voc√™ deve ver o estudante inclu√≠do na lista de usu√°rios:

![alt text](sb-usuario-incluido.png)

# 5. Conclus√£o

Neste tutorial, implementamos um integrador que consome eventos de CRUD do Redis e transforma os dados entre os modelos ORM/SQLite e ODM/MongoDB. O integrador foi projetado para ser modular e extens√≠vel, permitindo a adi√ß√£o de novos tipos de eventos e entidades no futuro.

V√°rios padr√µes de integra√ß√£o foram utilizados, como o padr√£o de publicador/assinante para eventos no Redis e o padr√£o de transforma√ß√£o de dados para converter os modelos entre os sistemas. A implementa√ß√£o do integrador foi feita utilizando Java, ORMLite para persist√™ncia no SQLite e Lettuce para comunica√ß√£o com o Redis. Tamb√©m foi utilizado o Apache Camel para gerenciar as rotas de transforma√ß√£o de dados e facilitar a integra√ß√£o entre os sistemas.

Como exerc√≠cio ou para futuras vers√µes deste tutorial, voc√™ pode explorar as seguintes ideias:

1. Implementar testes automatizados para o integrador, garantindo que as funcionalidades de CRUD estejam sempre funcionando.
2. Adicionar suporte a outros tipos de eventos no Redis, como atualiza√ß√£o e exclus√£o de estudantes.
3. Criar rotas de transforma√ß√£o do SB para o SGA, permitindo que os dados do SB sejam sincronizados com o SGA.


# Refer√™ncias

1. <a id="hohpe2004"></a>HOHPE, Gregor; WOOLF, Bobby. Enterprise integration patterns: Designing, building, and deploying messaging solutions. [S.l.]: Addison-Wesley Professional, 2004. 
